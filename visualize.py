#!/usr/bin/env python3

"""
Script to decode tokenized video into images/video.
Example usage: See https://github.com/1x-technologies/1xgpt?tab=readme-ov-file#1x-genie-baseline
"""

import argparse
import math
import os
from PIL import Image, ImageDraw

import numpy as np
import torch
import torch.distributed.optim
import torch.utils.checkpoint
import torch.utils.data
import torchvision.transforms.v2.functional as transforms_f
from einops import rearrange
from matplotlib import pyplot as plt

from data import RawTokenDataset
from magvit2.config import VQConfig
from magvit2.models.lfqgan import VQModel
from train_depth.Magvit2Dpt import Magvit2Dpt

from track_visualize import TrackVisualizer, track_chunks

FOV = 10
VIEW_MATRIX = [
    0.7232445478439331, -0.030260592699050903, 0.6899287700653076, 0.0, 
    0.5141233801841736, 0.6906105875968933, -0.5086592435836792, 0.0, 
    -0.46107974648475647, 0.7225935459136963, 0.5150379538536072, 0.0, 
    0.21526622772216797, -0.26317155361175537, -4.399168968200684, 1.0,
]

def parse_args():
    parser = argparse.ArgumentParser(description="Visualize tokenized video as GIF or comic.")
    parser.add_argument(
        "--stride",
        type=int,
        default=1,
        help="Frame skip",
    )
    parser.add_argument(
        "--token_dir",
        type=str,
        default="data/genie_generated",
        help="Directory of tokens, in the format of `video.bin` and `metadata.json`. "
             "Visualized gif and comic will be written here.",
    )
    parser.add_argument(
        "--tokenizer_ckpt",
        type=str,
        default="data/magvit2.ckpt",
        help="Path of the ckpt file of magvit2"
    )
    parser.add_argument(
        "--depth_decoder_ckpt",
        type=str,
        default=None,
        help="Path of the ckpt file of depth decoder"
    )
    parser.add_argument(
        "--offset", type=int, default=0, help="Offset to start generating images from"
    )
    parser.add_argument(
        "--fps", type=int, default=2, help="Frames per second"
    )
    parser.add_argument(
        "--max_images", type=int, default=None, help="Maximum number of images to generate. None for all."
    )
    parser.add_argument(
        "--track", action="store_true",
    )
    parser.add_argument(
        "--draw_point_cloud", action="store_true",
    )
    parser.add_argument(
        "--generated_data", action="store_true",
        help="Comic generation assumes `token_dir` follows the same format as generate: e.g., "
             "`prompt | predictions | gtruth` in `video.bin`, `window_size` in `metadata.json`."
             "Therefore, comic should be enabled when visualizing videosgenerated by the model"
    )
    args = parser.parse_args()

    return args


def export_to_gif(frames: list, output_gif_path: str, fps: int):
    """
    Export a list of frames to a GIF.

    Args:
    - frames (list): List of frames (as numpy arrays or PIL Image objects).
    - output_gif_path (str): Path to save the output GIF.
    - fps (int): Desired frames per second.
    """
    # Convert numpy arrays to PIL Images if needed
    pil_frames = [Image.fromarray(frame) if isinstance(
        frame, np.ndarray) else frame for frame in frames]

    duration_ms = 1000 / fps
    pil_frames[0].save(output_gif_path.replace(".mp4", ".gif"),
                       format="GIF",
                       append_images=pil_frames[1:],
                       save_all=True,
                       duration=duration_ms,
                       loop=0)


def rescale_magvit_output(magvit_output):
    """
    [-1, 1] -> [0, 255]

    Important: clip to [0, 255]
    """
    rescaled_output = ((magvit_output.detach().cpu() + 1) * 127.5)
    clipped_output = torch.clamp(rescaled_output, 0, 255).to(dtype=torch.uint8)
    return clipped_output


def decode_latents_wrapper(
        batch_size=16, 
        tokenizer_ckpt='data/magvit2.ckpt', 
        depth_decoder_ckpt=None, 
        genie_config=None,
        window_size=16,
        latent_side_len=25,
        max_images=None
    ):
    device = "cuda"

    magvit_config = VQConfig()
    magvit = VQModel(magvit_config, ckpt_path=tokenizer_ckpt)
    magvit = magvit.to(device=device, dtype=torch.bfloat16)

    if depth_decoder_ckpt is None:
        magvit2dpt = None
    else:
        magvit2dpt = Magvit2Dpt(
            genie_config=genie_config,
            maskgit_steps=1,
            window_size=window_size,
            latent_side_len=latent_side_len,
            use_genie=False,
            freeze_dpt=True,
        ).to(device)
        magvit2dpt.load_state_dict(torch.load(depth_decoder_ckpt))

    @torch.no_grad()
    def decode_latents(video_data):
        """
        video_data: (b, h, w), where b is different from training/eval batch size.
        """
        decoded_imgs = []
        decoded_depth = []

        for shard_ind in range(math.ceil(len(video_data) / batch_size)):
            batch = torch.from_numpy(video_data[shard_ind * batch_size: (shard_ind + 1) * batch_size].astype(np.int64))
            if magvit.use_ema:
                with magvit.ema_scope():
                    quant = magvit.quantize.get_codebook_entry(rearrange(batch, "b h w -> b (h w)"),
                                                              bhwc=batch.shape + (magvit.quantize.codebook_dim,)).flip(1)
                    quant = quant.to(device=device, dtype=torch.bfloat16)
                    decoded_imgs.append(((rescale_magvit_output(magvit.decode(quant)))))
                    if magvit2dpt is not None:
                        decoded_depth.append(magvit2dpt.decode(quant))
            if max_images and len(decoded_imgs) * batch_size >= max_images:
                break

        decoded_imgs = [img.permute(1, 2, 0).numpy() for img in torch.cat(decoded_imgs)]
        if magvit2dpt is not None:
            decoded_depth = transforms_f.resize(torch.cat(decoded_depth), decoded_imgs[0].shape[:-1])
            decoded_depth = [depth.cpu().numpy() for depth in decoded_depth]
        return decoded_imgs, decoded_depth

    return decode_latents


def caption_image(pil_image: Image, caption: str):
    """
    Add a bit of empty space at the top, and add the caption there
    """
    border_size = 36
    font_size = 24

    width, height = pil_image.size
    new_width = width
    new_height = height + border_size

    new_image = Image.new("RGB", (new_width, new_height), "white")
    new_image.paste(pil_image, (0, border_size))

    # Draw the caption
    draw = ImageDraw.Draw(new_image)

    # Center text (`align` keyword doesn't work)
    _, _, text_w, text_h = draw.textbbox((0, 0), caption, font_size=font_size)
    draw.text(((width - text_w) / 2, (border_size - text_h) / 2), caption, fill="black", font_size=font_size)

    return new_image

def combine_rgb_depth(rgb_frames, depth_frames):
    combined_frames = []
    for rgb_frame, depth_frame in zip(rgb_frames, depth_frames):
        plt.imshow(depth_frame)
        plt.axis('off')
        plt.savefig('temp_depth.png', bbox_inches='tight', pad_inches=0)
        plt.close()
        pseudo_color_image = Image.open('temp_depth.png')
        rgb_image_pil = Image.fromarray(rgb_frame)
        pseudo_color_image = pseudo_color_image.resize(rgb_image_pil.size)
        combined_image = Image.new('RGB', (rgb_image_pil.width + pseudo_color_image.width, rgb_image_pil.height))
        combined_image.paste(rgb_image_pil, (0, 0))
        combined_image.paste(pseudo_color_image, (rgb_image_pil.width, 0))
        combined_frames.append(combined_image)
    return combined_frames

def deproject(fov, view_matrix, depth_img):
    """
    Deprojects a pixel point to 3D coordinates
    Args
        fov: fov angle of the camera, in degree
        view_matrix
        depth_img: np.array; depth image used as reference to generate 3D coordinates
    Output
        (x, y, z): (3, npts) np.array; world coordinates of the deprojected point
    """
    h, w = depth_img.shape
    u, v = np.meshgrid(np.arange(w), np.arange(h)) # coordinates of each pixel
    u, v = u.ravel(), v.ravel() # reshaped to 1d list

    # The OpenGV view matrix is a 1-dim list needed to be reshaped and transposed
    T_world_cam = np.linalg.inv(np.array(view_matrix).reshape((4, 4)).T) # transform from cam to world
    z = depth_img[v, u] # depth of each pixel 

    """
                     object height
                      __________
                      |        /
                      |       / 
                      |      /  
                   z  |     /   
                      |    /    
                      |   /
                      |  /
                      | /       
                      |/
                     /|
            FOV/2 ->/ |
                   /  |
                  /   |
                 /    | 
                /     | focal length (foc)
               /      |
              /       |
             /        |
            /_________|
          image height
    """

    foc = h / (2 * np.tan(np.deg2rad(fov) / 2)) # focal length
    x = (u - w // 2) * z / foc # object real width
    y = -(v - h // 2) * z / foc # object real hight
    z = -z

    ones = np.ones_like(z) # this additional column does translation with mulplication
    cam_pos = np.stack([x, y, z, ones], axis=0)
    world_pos = T_world_cam @ cam_pos

    return world_pos[:3]

def draw_pcd(rgb_frames, depth_frames):
    pcd_imgs = []
    for rgb_frame, depth_frame in zip(rgb_frames, depth_frames):
        world_pos = deproject(
            FOV,
            VIEW_MATRIX,
            depth_frame,
        ).T
        rgb = rgb_frame / 255.
        rgb = rearrange(rgb, 'h w c -> (h w) c')

        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')
        ax.view_init(25, -75)
        ax.scatter(world_pos[:,0], world_pos[:,1], world_pos[:,2], s=1, c=rgb)
        ax.proj_type = 'ortho'
        fig.canvas.draw()
        w, h = fig.canvas.get_width_height()
        img_data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)
        img_data = img_data.reshape((h, w, 3))
        pcd_imgs.append(Image.fromarray(img_data))
        plt.close(fig)
    return pcd_imgs

@torch.no_grad()
def main():
    args = parse_args()

    # Load tokens
    token_dataset = RawTokenDataset(args.token_dir, 1, filter_interrupts=False, filter_overlaps=False)
    video_tokens = token_dataset.data
    metadata = token_dataset.metadata
    output_gif_path = os.path.join(args.token_dir, f"generated_offset{args.offset}.gif")

    video_frames, depth_frames = decode_latents_wrapper(
        max_images=args.max_images, 
        tokenizer_ckpt=args.tokenizer_ckpt,
        depth_decoder_ckpt=args.depth_decoder_ckpt,
    )(video_tokens[args.offset::args.stride])

    # `generate` should populate `metadata.json` with these keys, while ground truth metadata does not have them
    is_generated_data = all(key in metadata for key in ("num_prompt_frames", "window_size"))
    if args.track:
        device = torch.device("cuda")
        cotracker = torch.hub.load("facebookresearch/co-tracker", "cotracker2_online").to(device)
        vis = TrackVisualizer(
            save_dir="./saved_videos", 
            linewidth=0.5,
            mode='cool',
        )
        if is_generated_data:
            pred, gen_track_frames = track_chunks(
                cotracker, 
                vis,
                video_frames[metadata["num_prompt_frames"]:metadata["window_size"]], 
                video_chunk_max_len=metadata["window_size"]-metadata["num_prompt_frames"], 
                threshold_perc=0.2, 
                grid_size=50,
            )
            pred, gtruth_track_frames = track_chunks(
                cotracker, 
                vis,
                video_frames[metadata["window_size"]:], 
                video_chunk_max_len=metadata["window_size"]-metadata["num_prompt_frames"], 
                threshold_perc=0.2, 
                grid_size=50,
            )
            video_frames = video_frames[:metadata["num_prompt_frames"]] + gen_track_frames + gtruth_track_frames
        else:
            pred, gtruth_track_frames = track_chunks(
                cotracker, 
                vis,
                video_frames, 
                video_chunk_max_len=metadata["window_size"]-metadata["num_prompt_frames"], 
                threshold_perc=0.2, 
                grid_size=50,
            )
            video_frames = video_frames[:metadata["num_prompt_frames"]] + gtruth_track_frames
    if args.depth_decoder_ckpt is not None:
        if args.draw_point_cloud and not args.track:
            video_frames = draw_pcd(video_frames, depth_frames)
        else:
            video_frames = combine_rgb_depth(video_frames, depth_frames)
    if is_generated_data:
        if video_tokens.shape[0] != metadata["window_size"] * 2 - metadata["num_prompt_frames"]:
            raise ValueError(f"Unexpected {video_tokens.shape=} given {metadata['window_size']=}, {metadata['num_prompt_frames']=}")

        captioned_frames = []
        for i, frame in enumerate(video_frames):
            if i < metadata["num_prompt_frames"]:
                caption = "Prompt"
            elif i < metadata["window_size"]:
                caption = "Generated"
            else:
                caption = "Ground truth"

            captioned_frames.append(caption_image(frame, caption))
    else:
        # Leave ground truth frames uncaptioned
        captioned_frames = video_frames

    export_to_gif(captioned_frames, output_gif_path, args.fps)
    print(f"Saved to {output_gif_path}")

    if args.generated_data:
        fig, axs = plt.subplots(nrows=2, ncols=metadata["window_size"], figsize=(3 * 2 * metadata["window_size"], 3 * 2))
        for i, image in enumerate(video_frames):
            if i < metadata["num_prompt_frames"]:
                curr_axs = [axs[0, i], axs[1, i]]
                title = "Prompt"

            elif i < metadata["window_size"]:
                curr_axs = [axs[0, i]]
                title = "Prediction"
            else:
                curr_axs = [axs[1, i - metadata["window_size"] + metadata["num_prompt_frames"]]]
                title = "Ground truth"

            for ax in curr_axs:
                ax.set_title(title)
                ax.imshow(image)
                ax.axis("off")

        output_comic_path = os.path.join(args.token_dir, f"generated_comic_offset{args.offset}.png")
        plt.savefig(output_comic_path, bbox_inches="tight")
        plt.close()
        print(f"Saved to {output_comic_path}")


if __name__ == "__main__":
    main()
